# Исправление: Увеличены таймауты для Ollama

## Проблема

Бот показывал "typing..." но не отправлял ответ.

**Причина:** Первый запрос к Ollama занимает 30-60+ секунд (модель загружается в память), но таймаут был всего 30 секунд.

---

## Решение

### Увеличены таймауты в `src/ai/ollama_client.py`

**Было:**
```python
timeout=aiohttp.ClientTimeout(total=30)  # 30 секунд
```

**Стало:**
```python
timeout=aiohttp.ClientTimeout(total=90)  # 90 секунд
```

Изменено в двух местах:
- `generate()` - строка 97
- `chat()` - строка 147

---

## Как это работает

### Первый запрос (медленно):
1. Бот получает сообщение "Привет, как дела?"
2. Отправляет запрос в Ollama
3. Ollama загружает модель в память (~30-60 сек)
4. Генерирует ответ (~5-10 сек)
5. **Общее время: 40-70 секунд**

### Последующие запросы (быстро):
1. Модель уже в памяти
2. Генерация ответа ~3-10 секунд
3. **Общее время: 3-10 секунд**

---

## Что изменилось

| Параметр | До | После |
|----------|-----|--------|
| Таймаут generate | 30 сек | 90 сек |
| Таймаут chat | 30 сек | 90 сек |
| Первый ответ | ❌ Таймаут | ✅ Успех |
| Последующие ответы | ✅ Быстро | ✅ Быстро |

---

## Тестирование

Откройте бота в Telegram и попробуйте:

### Тест 1: Первый вопрос (будет медленно)
```
Вы: Привет, как дела?
Ожидание: 40-60 секунд, потом придет ответ ✅
```

**Это нормально!** Первый запрос всегда медленный.

### Тест 2: Второй вопрос (будет быстро)
```
Вы: Какие услуги вы предоставляете?
Ожидание: 3-10 секунд ✅
```

### Тест 3: Продолжение диалога (быстро)
```
Вы: Расскажи подробнее
Ожидание: 3-10 секунд ✅
```

---

## Как ускорить первый запрос

### Вариант 1: Держать Ollama "теплой"

После запуска бота выполните тестовый запрос:

```powershell
& "C:\Users\$env:USERNAME\AppData\Local\Programs\Ollama\ollama.exe" run llama3.2:3b "Привет"
```

Это загрузит модель в память заранее.

### Вариант 2: Использовать меньшую модель

```powershell
ollama pull llama3.2:1b  # Меньше, быстрее
```

Потом в `.env`:
```env
OLLAMA_MODEL=llama3.2:1b
```

Качество будет немного хуже, но первый запрос быстрее (~20-30 сек).

---

## Дополнительная оптимизация

### Можно добавить "прогрев" модели при старте бота

В `src/main.py` после подключения к Ollama:

```python
if ollama_client and await ollama_client.check_health():
    print("[OK] Ollama is available")
    
    # Прогреваем модель
    print("[INFO] Warming up Ollama model...")
    await ollama_client.generate("Привет")
    print("[OK] Model is ready")
```

Это увеличит время запуска бота, но первый ответ пользователю будет быстрым.

---

## Статус

✅ **Исправлено:** Таймауты увеличены до 90 секунд
✅ **Бот перезапущен**
✅ **Готов к тестированию**

---

## Протестируйте сейчас!

Отправьте боту любой вопрос и подождите 40-60 секунд на первый ответ.

Последующие ответы будут быстрыми!

---

**Версия:** 0.4.1 - Timeout Fix  
**Дата:** 12 февраля 2026
